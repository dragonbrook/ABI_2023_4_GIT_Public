{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4055ff9",
   "metadata": {},
   "source": [
    "# Importing Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b45cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio import Align\n",
    "from Bio.Align import substitution_matrices\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "# import Bio.Entrez as ez\n",
    "# ez.email = \"ratlos@habmalnefrage.de\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2725b",
   "metadata": {},
   "source": [
    "# Setting Analysis Parameters\n",
    "## Adjust parameters to your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7c08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/john/Excercises/Sequence_Analysis/Protein_Blast/\" # Adjust your working directory\n",
    "dataset_index=1 # Choose a set of data to analyse, the higher the number, the longer calculation will take\n",
    "dataset=[[\"query_seq.fasta\",\"synthetic_seqs.fasta\"],[\"NP_477249.fasta\",\"seqdump.txt\"],[\"Q72GW4.fasta\",\"EF-Tu_BLAST.fasta\"],[\"Q72GW4.fasta\",\"uniprotkb_proteome_UP000000625_AND_revi_2024_01_23.fasta\"]]\n",
    "query=dataset[dataset_index][0]\n",
    "database_file=dataset[dataset_index][1]\n",
    "kmer = 3\n",
    "tscore = 13 # default 13\n",
    "aligner = Align.PairwiseAligner\n",
    "gap_open = 10 # default 10\n",
    "gap_extend = 2 #default 2\n",
    "joining_threshold = 36 # default 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75724355",
   "metadata": {},
   "source": [
    "# Choose and Load Scoring Matrix\n",
    "## Adjust parameters to your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab6f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOSUM90_df\n"
     ]
    }
   ],
   "source": [
    "avaliable_scoring_matrices=[\"BLOSUM90\",\"BLOSUM45\",\"PAM120\"]\n",
    "index_of_scoring_matrix=0 # number refers to the list index of the matrix of choice from \"avaliable_scoring_matrices\"\n",
    "scoringmatrix_df = pd.read_csv(f\"{path}{avaliable_scoring_matrices[index_of_scoring_matrix]}.csv\", index_col=0) # Number format must be compatible with int8 or the script will not run\n",
    "scoringmatrix = scoringmatrix_df.to_dict(orient='dict', index=True) # put the name of any available scoringmatrix_dict here\n",
    "matrix_for_local_alignments = \"BLOSUM45\"\n",
    "matrix_for_global_alignments = \"BLOSUM45\"\n",
    "local_score_threshold = 15\n",
    "global_score_threshold = 50\n",
    "print(f\"{avaliable_scoring_matrices[index_of_scoring_matrix]}_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd7430",
   "metadata": {},
   "source": [
    "# Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b4e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "len_query=0\n",
    "name_query=\"\"\n",
    "seq_query=\"\"\n",
    "job_ID=\"\"\n",
    "len_query_minus_kmer=0\n",
    "cores=multiprocessing.cpu_count()\n",
    "\n",
    "# Lists\n",
    "database_items=[]\n",
    "db_entries=[]\n",
    "hit_IDs=[]\n",
    "selected_hit_IDs=[]\n",
    "\n",
    "# Dictionaries\n",
    "database_dict={}\n",
    "scan_dict={}\n",
    "kmer_dict={}\n",
    "diagonals_dict={}\n",
    "top10_dict={}\n",
    "nolh_dict={}\n",
    "chains_dict={}\n",
    "locals_dict={}\n",
    "globals_dict={}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09386d",
   "metadata": {},
   "source": [
    "# Parsing Query Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116e9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query_sequence(path, query):\n",
    "    print(\"Parsing query sequence\")\n",
    "    with open(f\"{path}query\") as handle:\n",
    "        for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "            print(\"Query:\")\n",
    "            print(seq_record.id)\n",
    "            print(\"Query Sequence:\", repr(seq_record.seq))\n",
    "            print(\"Query sequence length:\", len(seq_record))\n",
    "            print()\n",
    "            len_query = len(seq_record)\n",
    "            name_query = seq_record.id\n",
    "            seq_query = seq_record.seq\n",
    "            return name_query, len_query, seq_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83106cb6",
   "metadata": {},
   "source": [
    "# Creating Job-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4f4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_job_ID(name_query):\n",
    "    job_id=str(name_query+datetime.now().strftime('%Y%m-%d%H-%M%S-'))\n",
    "    return(job_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e345dc9",
   "metadata": {},
   "source": [
    "# Parsing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15bcf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_patabase(path, database_file):\n",
    "    print(\"Parsing database\")\n",
    "    db_entries=[]\n",
    "    sequence_count=0\n",
    "    with open(f\"{path}database_file\") as handle: # this is a fasta file containing 127 sequences coming from an NCBI blast of the query sequence\n",
    "        for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "            sequence_count+=1\n",
    "            database_items.append(seq_record.id)\n",
    "            database_dict[f\"ID_{seq_record.id}\"] = seq_record.id\n",
    "            database_dict[f\"Length_{seq_record.id}\"] = len(seq_record)\n",
    "            database_dict[f\"Sequence_{seq_record.id}\"] = seq_record.seq\n",
    "            scan_dict[f\"Scan_hits_{seq_record.id}\"] = pd.DataFrame(index=range(len(seq_record) - kmer + 1), columns=range(len_query - kmer + 1)).fillna(False)\n",
    "            scan_dict[f\"Scan_diagonals_{seq_record.id}\"] = pd.DataFrame(index=range(len(seq_record) - kmer + 1), columns=range(len_query - kmer + 1)).fillna(0)\n",
    "            scan_dict[f\"Scan_diagonals_{seq_record.id}\"] = scan_dict[f\"Scan_diagonals_{seq_record.id}\"].astype('int16')\n",
    "            db_entries.append(seq_record.id)\n",
    "    print(sequence_count,\"sequences parsed\")\n",
    "    return(db_entries, database_dict, scan_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59bc6",
   "metadata": {},
   "source": [
    "# Creating K-mers and Neighbors Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c6ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kmers(kmer, len_query, seq_query):\n",
    "    aa=[\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"V\",\"W\",\"X\",\"Y\"]\n",
    "\n",
    "    # Creating Kmers dictionary: Making a dictionary containing all possible single-point mutations of a given kmer\n",
    "    kmer_dict = {}\n",
    "    print(\"Defining K-mers and all possible single-point mutants\")\n",
    "    for i in range(len_query - kmer):\n",
    "        kmer_dict[f\"kmer_{i}\"] = [str(seq_query[i:i+kmer]), 0]\n",
    "        for j in range(kmer):\n",
    "            for k in range(len(aa)):\n",
    "                peptide = kmer_dict[f\"kmer_{i}\"][0]\n",
    "                mutant = \"\"\n",
    "                for l in range(kmer):\n",
    "                    if l != j:\n",
    "                        mutant += peptide[l]\n",
    "                    else:\n",
    "                        mutant += aa[k]\n",
    "                kmer_dict[f\"{i}_{j}_{k}\"] = [mutant, 0]\n",
    "    print(\"Creation of K-Mers Completed\")\n",
    "#     df = pd.DataFrame.from_dict(kmer_dict, orient='index', columns=['Sequence', 'Score'])\n",
    "#     print(df)\n",
    "    # Scoring K-mers: \n",
    "    print(\"Scoring K-mers\")\n",
    "    for i in range(len_query-kmer):\n",
    "        for j in range(kmer):\n",
    "            score = 0\n",
    "            score += scoringmatrix[kmer_dict[f\"kmer_{i}\"][0][j]][kmer_dict[f\"kmer_{i}\"][0][j]]\n",
    "            kmer_dict[f\"kmer_{i}\"][1] = score\n",
    "            for k in range(len(aa)):\n",
    "                score += scoringmatrix[kmer_dict[f\"kmer_{i}\"][0][j]][kmer_dict[f\"{i}_{j}_{k}\"][0][j]]\n",
    "                kmer_dict[f\"{i}_{j}_{k}\"][1] = score\n",
    "#    df = pd.DataFrame.from_dict(kmer_dict, orient='index', columns=['Sequence', 'Score'])\n",
    "#    print(df)\n",
    "    return kmer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950fc89",
   "metadata": {},
   "source": [
    "# Creating Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e00c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diagonals(database_dict, kmer_dict, tscore, db_entries):\n",
    "    diagonals_dict=defaultdict(dict)\n",
    "    print(\"Creating diagonals\")\n",
    "    direct_hits=0\n",
    "    mismatched_hits=0\n",
    "    for i in range(len(db_entries)):\n",
    "        diagonals_dict[db_entries[i]] = defaultdict(list)\n",
    "        hitstart_db=0\n",
    "        hitstart_query=0\n",
    "        score=0\n",
    "        db_seq=str(database_dict[f\"Sequence_{db_entries[i]}\"])\n",
    "        for j in range(len_query-kmer):\n",
    "            for k in range(len(db_seq)-kmer):\n",
    "                if kmer_dict[f\"kmer_{j}\"][0]==db_seq[k:k+kmer]: # Search for a direct seed hit\n",
    "                    direct_hits +=1\n",
    "                    score = kmer_dict[f\"kmer_{j}\"][1]\n",
    "                    hitstart_query = j # index of the first amino acid in the query sequence matching the database sequence\n",
    "                    hitstart_db = k # index of the first amino acid in the database sequence matching the first amino acid of the tested kmer\n",
    "                    hitend_query = j+kmer # index of the last amino acid in the query sequence matched to the database sequence\n",
    "                    hitend_db = k+kmer # index of the last amino acid in the database sequence matched to the tested kmer\n",
    "                    # Scan forward\n",
    "                    query_pos = j\n",
    "                    db_pos = k\n",
    "                    hit_seq = kmer_dict[f\"kmer_{j}\"][0]\n",
    "                    while query_pos < (len_query-kmer-1) and db_pos < len(db_seq)-kmer-1:\n",
    "                        query_pos += 1\n",
    "                        db_pos += 1\n",
    "                        if kmer_dict[f\"kmer_{query_pos}\"][0]==db_seq[db_pos:db_pos+kmer]: # try expanding the seed hit with another direct hit\n",
    "                            hitend_query += 1 # update coordinates\n",
    "                            hitend_db += 1 # update coordinates\n",
    "                            direct_hits += 1\n",
    "                            hit_seq = hit_seq + kmer_dict[f\"kmer_{query_pos}\"][0][-1] # add the last letter of the matching k-mer to the hit sequence\n",
    "                            score += kmer_dict[f\"kmer_{query_pos}\"][1] # update the score of the expanded seed\n",
    "                        else: # try expanding the seed hit with a mismatched, positive scored kmer\n",
    "                            for l in range(kmer):\n",
    "                                for m in range(len(aa)):\n",
    "                                    if kmer_dict[f\"{query_pos}_{l}_{m}\"][1]>0:\n",
    "                                        if kmer_dict[f\"{query_pos}_{l}_{m}\"][0]==db_seq[db_pos:db_pos+kmer]:\n",
    "                                            hitend_query += 1\n",
    "                                            hitend_db += 1\n",
    "                                            mismatched_hits += 1\n",
    "                                            hit_seq = hit_seq + kmer_dict[f\"{query_pos}_{l}_{m}\"][0][-1] # add the last letter of the matching k-mer to the hit sequence\n",
    "                                            score += kmer_dict[f\"{query_pos}_{l}_{m}\"][1] # update the score of the expanded seed\n",
    "                                        else:\n",
    "                                            break\n",
    "                    # Scan backward\n",
    "                    query_pos = j\n",
    "                    db_pos = k\n",
    "                    while query_pos > 1 and db_pos > 1:\n",
    "                        query_pos -= 1\n",
    "                        db_pos -= 1\n",
    "                        if kmer_dict[f\"kmer_{query_pos}\"][0]==db_seq[db_pos:db_pos+kmer]:\n",
    "                            hitstart_query -= 1\n",
    "                            hitstart_db -= 1\n",
    "                            direct_hits += 1\n",
    "                            hit_seq = kmer_dict[f\"kmer_{query_pos}\"][0][0] + hit_seq \n",
    "                            score += kmer_dict[f\"kmer_{query_pos}\"][1]\n",
    "                        else:\n",
    "                            for l in range(kmer):\n",
    "                                for m in range(len(aa)):\n",
    "                                    if kmer_dict[f\"{query_pos}_{l}_{m}\"][1]>0:\n",
    "                                        if kmer_dict[f\"{query_pos}_{l}_{m}\"][0]==db_seq[db_pos:db_pos+kmer]:\n",
    "                                            hitstart_query -= 1\n",
    "                                            hitstart_db -= 1\n",
    "                                            mismatched_hits += 1\n",
    "                                            hit_seq = hit_seq + kmer_dict[f\"{query_pos}_{l}_{m}\"][0][-1]\n",
    "                                            score += kmer_dict[f\"{query_pos}_{l}_{m}\"][1]\n",
    "                                        else:\n",
    "                                            break\n",
    "                    # Store hits\n",
    "                    if score >= tscore:\n",
    "                        diagonals_dict[db_entries[i]][f\"{hitstart_db}_{hitend_db}\"] = [score, hitstart_query, hitend_query, hitstart_db, hitend_db, hit_seq]\n",
    "    print(\"Diagonals created\")\n",
    "#    print(\"Direct hits found:\",direct_hits)\n",
    "#    print(\"Mismatched hits found:\",mismatched_hits)\n",
    "    kmer_dict={} # Purge kmer_dict\n",
    "    return diagonals_dict, kmer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57319",
   "metadata": {},
   "source": [
    "# Filtering the 10 Best Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f551c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top10(diagonals_dict):\n",
    "    print(\"Filtering best diagonals\")\n",
    "    print(\"Top 10 hits above threshold for each database sequence:\")\n",
    "    top10_dict = defaultdict(dict)\n",
    "    for db_entry, inner_dict in diagonals_dict.items():\n",
    "        diagonals_for_entry = list(inner_dict.values())\n",
    "        if diagonals_for_entry:\n",
    "            diagonals_for_entry.sort(key=lambda x: x[0], reverse=True)\n",
    "            top10_dict[db_entry] = defaultdict(list)\n",
    "            top10_dict[db_entry] = diagonals_for_entry[:min(len(diagonals_for_entry), 10)]\n",
    "    diagonals_dict={} # Purge diagonals_dict\n",
    "    return top10_dict, diagonals_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49609051",
   "metadata": {},
   "source": [
    "# Filtering Non-Overlapping Hits (NOLH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7306859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nolh(top10_dict):\n",
    "    print(\"Filtering best non-overlapping hits\")\n",
    "    nolh_dict = defaultdict(dict)\n",
    "    for db_entry, inner_list in top10_dict.items():\n",
    "        nolh = []\n",
    "        for values in inner_list:\n",
    "            if not any(\n",
    "                (\n",
    "                    values[3] in range(hit[3], hit[4] + 1)\n",
    "                    or values[4] in range(hit[3], hit[4] + 1)\n",
    "                    or values[1] in range(hit[1], hit[2] + 1)\n",
    "                    or values[2] in range(hit[1], hit[2] + 1)\n",
    "                )\n",
    "                for hit in nolh\n",
    "            ):\n",
    "                nolh.append(values)\n",
    "        nolh.sort(key=lambda x: x[0], reverse=True)\n",
    "        nolh_dict[db_entry] = nolh\n",
    "    return nolh_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fba060",
   "metadata": {},
   "source": [
    "# Chaining Non-Overlapping Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf22c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaining(nolh_dict, joining_threshold):\n",
    "    def calculate_joining_penalty(db_end,db_start, joining_threshold):\n",
    "        gapsize = db_start - db_end\n",
    "        penalty = 0\n",
    "        if gapsize > 1:\n",
    "            penalty = gap_open + gapsize * gap_extend\n",
    "            if penalty < joining_threshold:\n",
    "                return True, gapsize, penalty # do this if there is a gap, but the penalty within agreeable limits\n",
    "            else:\n",
    "                return False, gapsize, penalty # do this if the gap is too large\n",
    "        else:\n",
    "            return True, gapsize, penalty # do this if there is no gap\n",
    "\n",
    "    print()\n",
    "    print(\"Chaining non-overlapping hits\")\n",
    "    chains_dict=defaultdict(dict)\n",
    "    joining_events=0\n",
    "    for db_entry, inner_list in nolh_dict.items():\n",
    "        hits=inner_list\n",
    "        hits.sort(key=lambda x: x[3], reverse=False) # Sort by hit start position in the database sequence\n",
    "        segment=\"\"\n",
    "        chain=[]\n",
    "        shackle = 0\n",
    "        segment_score = 0\n",
    "        gapsize = 0\n",
    "        while shackle < len(hits):\n",
    "            if len(hits) - shackle >1: # If there is another element in the list after the current one, check if the two elements can be joined\n",
    "                join, gapsize, penalty = calculate_joining_penalty(hits[shackle][4],hits[shackle+1][3]) # Arguments handed over: end of current hit and start of next hit\n",
    "                segment += hits[shackle][5] # add the sequence string\n",
    "                if join:\n",
    "                    segment_score += hits[shackle][0] - penalty # add match score and apply penalty for gaps to joined segments\n",
    "                    for i in range(gapsize): # filling gaps with '-', the sequence of the next non-overlapping hit will be added in the next loop iteration\n",
    "                       segment += \"-\"\n",
    "                    joining_events += 1\n",
    "                    shackle += 1 # Update loop variable\n",
    "                    continue\n",
    "                else:\n",
    "                    segment_score += hits[shackle][0] # add match score\n",
    "                    chain.append([segment_score, segment])\n",
    "                    shackle += 1 # Update loop variable\n",
    "            else: # If this is the last element in the list, append the last element and end the loop\n",
    "               segment += hits[shackle][5]\n",
    "               segment_score += hits[shackle][0]\n",
    "               chain.append([segment_score, segment])\n",
    "               break\n",
    "        chain.sort(key=lambda x: x[0], reverse=True) # sort chains by score\n",
    "        chains_dict[f\"{db_entry}\"]=chain[0]\n",
    "        print(\"Target ID:\",db_entry,\"Best scoring chain:\",chain[0])\n",
    "\n",
    "    # df = pd.DataFrame.from_dict(chains_dict, orient='index', columns=['Score','Sequence'])\n",
    "    # print(df)\n",
    "    print(\"Number of segments joined:\",joining_events)\n",
    "    return chains_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d4e27",
   "metadata": {},
   "source": [
    "# Writing Hit Sequences to Fasta File and Parsing of Hit-File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_parse_hitfile(path, job_ID, chains_dict):\n",
    "    print(\"Writing chains to hitfile.fasta\")\n",
    "    hitfile = open(f\"{path}/output/{job_ID}_hitfile.fasta\", \"w\")\n",
    "    for db_entry, value in chains_dict.items(): # db_entry refers to the database sequence that produced a hit\n",
    "        if value[1]!=\"\":\n",
    "    #         print(db_entry, value[0])\n",
    "    #         print(db_entry, value[1])\n",
    "            hitfile.write(\">\" + db_entry + \" \" + \"\\n\" + value[1]+ \"\\n\"+\"\\n\")\n",
    "    hitfile.close()\n",
    "\n",
    "    # Parsing hitfile for subsequent alignment\n",
    "    aligner=Align.PairwiseAligner()\n",
    "    hitfile = f\"{path}/output/{job_ID}_hitfile.fasta\"\n",
    "\n",
    "    hit_IDs=[]\n",
    "    hitparse_dict={}\n",
    "    print(\"Parsing hitfile\")\n",
    "    with open(hitfile) as handle:\n",
    "        for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "            hit_IDs.append(seq_record.id)\n",
    "            hitparse_dict[f\"ID_{seq_record.id}\"] = seq_record.id\n",
    "            hitparse_dict[f\"Length_{seq_record.id}\"] = len(seq_record)\n",
    "            hitparse_dict[f\"Sequence_{seq_record.id}\"] = seq_record.seq\n",
    "    return hit_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82790afb",
   "metadata": {},
   "source": [
    "# Creating Local Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa23762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_alignment(path, job_ID, hit_IDs, chain_lightning, db_hit, local_matrix):\n",
    "    print(\"Creating local alignments for filtering\")\n",
    "    aligner.mode = \"local\"\n",
    "    aligner.open_gap_score = -10\n",
    "    aligner.extend_gap_score = -0.5\n",
    "    local_matrix = substitution_matrices.load(matrix_for_local_alignments)\n",
    "    local_alignments = aligner.align(chain_lightning, db_hit)\n",
    "    local_scores = aligner.score(chain_lightning, db_hit)\n",
    "    filename = f\"{path}/output/{job_ID}_Local_Alignments.txt\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for i in range(len(local_alignments)):\n",
    "            file.write(str(f\">{hit_ID} Score: {local_scores}\\n\"))\n",
    "            file.write(f\"{(str(local_alignments[i]))},'\\n','\\n'\")\n",
    "    return local_scores, local_alignments\n",
    "\n",
    "def call_local_alignment(path, job_ID, hit_IDs, hitparse_dict, database_dict, matrix_for_global_alignments):\n",
    "    locals_dict={}\n",
    "    selected_hit_IDs=[]\n",
    "    local_matrix=substitution_matrices.load(matrix_for_global_alignments)\n",
    "    for record in hit_IDs:\n",
    "        query=hitparse_dict[f\"Sequence_{record}\"]\n",
    "        target=database_dict[f\"Sequence_{record}\"]\n",
    "        local_scores, local_alignments = local_alignment(record, query, target, matrix_for_local_alignments)\n",
    "        locals_dict[f\"{record}\"]=[local_scores, local_alignments]\n",
    "        if local_scores > local_score_threshold:\n",
    "            selected_hit_IDs.append(record)\n",
    "            print(local_scores)\n",
    "            print(local_alignments[0])\n",
    "    return selected_hit_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaadfd7",
   "metadata": {},
   "source": [
    "# Creating Global Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24487855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_alignment(path, job_ID, hit_ID, seq_query, db_hit, global_matrix):\n",
    "    aligner.mode = \"global\"\n",
    "    aligner.open_gap_score = -10\n",
    "    aligner.extend_gap_score = -0.5\n",
    "\n",
    "    global_alignments = aligner.align(seq_query, db_hit)\n",
    "    global_scores = aligner.score(seq_query, db_hit)\n",
    "    return global_scores, global_alignments\n",
    "\n",
    "def call_global_alignment(path, job_ID, database_dict, selected_hit_IDs, global_score_threshold, seq_query, matrix_for_global_alignments):\n",
    "    print(\"Hit IDs:\",hit_IDs)\n",
    "    global_matrix = substitution_matrices.load(matrix_for_global_alignments)\n",
    "    globals_dict={}\n",
    "    for record in selected_hit_IDs:\n",
    "        target=database_dict[f\"Sequence_{record}\"]\n",
    "        global_scores, global_alignments = global_alignment(path, job_ID, record,seq_query,target,global_matrix)\n",
    "        if global_scores > global_score_threshold:\n",
    "            globals_dict[f\"{record}\"]=[global_scores, global_alignments]\n",
    "    return globals_dict\n",
    "            \n",
    "def save_global_alignments(job_ID, globals_dict):\n",
    "    filename = f\"{job_ID}_global.txt\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for i in range(len(global_alignments)):\n",
    "            file.write(str(f\">{record} Score: {global_scores}\\n\"))\n",
    "            file.write(f\"{(str(global_alignments[i]))},'\\n','\\n'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4739683",
   "metadata": {},
   "source": [
    "# Function to Call Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e4c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_cpu_count():\n",
    "#    return multiprocessing.cpu_count()\n",
    "\n",
    "def process_function(step):\n",
    "    function, use_multiprocessing, boundary, args, returns = step\n",
    "    print(function, use_multiprocessing, boundary, args, returns)\n",
    "    if use_multiprocessing:\n",
    "        chunk_size = max(1, boundary // multiprocessing.cpu_count())\n",
    "        chunks = [(i * chunk_size, (i + 1) * chunk_size) for i in range(get_cpu_count())]\n",
    "        chunks[-1] = (chunks[-1][0], boundary)\n",
    "\n",
    "        pool = multiprocessing.Pool(processes=get_cpu_count())\n",
    "        results = pool.starmap(function, [(args, chunk[0], chunk[1]) for chunk in chunks])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # Combine the results from different chunks\n",
    "        combined_result = []\n",
    "        for result in results:\n",
    "            combined_result.extend(result)\n",
    "\n",
    "        return combined_result\n",
    "\n",
    "    else:\n",
    "        return function(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1303a4d",
   "metadata": {},
   "source": [
    "# Defining the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61484c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69502c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function parse_query_sequence at 0x7f46f80d4400> False 1 ('/home/john/Excercises/Sequence_Analysis/Protein_Blast/', 'NP_477249.fasta') name_query, len_query, seq_query\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "parse_query_sequence() missing 1 required positional argument: 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(steps):\n\u001b[1;32m     19\u001b[0m     result_variable_names \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     results \u001b[38;5;241m=\u001b[39m process_function(step)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Assign results to variables based on the provided variable names\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, result_var_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result_variable_names):\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mprocess_function\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_result\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(args)\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_query_sequence() missing 1 required positional argument: 'query'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "# Syntax of \"steps\": 1. Name of function to be called | 2. Use multiprocessing for executing a function | \n",
    "# 3. Upper boundary of chunk size | 4. Arguments handed to the function | 5. Variables returned by the function\n",
    "    steps=[\n",
    "        [parse_query_sequence,False,1,(path, query),\"name_query, len_query, seq_query\"],\n",
    "        [generate_job_ID,False,1,(name_query),\"job_ID\"],\n",
    "        [parse_patabase,True,len(db_entries),(path, database_file),\"db_entries, database_dict, scan_dict\"],\n",
    "        [create_kmers,False,(len_query-kmer), (kmer, len_query, seq_query, scoringmatrix),\"kmer_dict\"],\n",
    "        [create_diagonals,True,len(db_entries),(database_dict, kmer_dict, tscore, db_entries),\"diagonals_dict, kmer_dict\"],\n",
    "        [filter_top10,True,len(diagonals_dict),(diagonals_dict),\"top10_dict, diagonals_dict\"],\n",
    "        [filter_nolh,True,len(top10_dict),(top10_dict),\"nolh_dict\"],\n",
    "        [chaining,True,len(nolh_dict),(nolh_dict, joining_threshold),\"chains_dict\"],\n",
    "        [create_and_parse_hitfile,False,1,(path, job_ID, chains_dict),\"\"],\n",
    "        [call_local_alignment,False,1,(path, job_ID, hit_IDs),\"\"],\n",
    "        [call_global_alignment,False,1,(path, job_ID, selected_hit_IDs),(path, job_ID, database_dict, selected_hit_IDs, global_score_threshold, seq_query, matrix_for_global_alignments),\"globals_dict\"],\n",
    "        [save_global_alignments,False, 1,(job_ID, globals_dict),\"\"]\n",
    "        ]\n",
    "    for i, step in enumerate(steps):\n",
    "        result_variable_names = step[4].split(\", \")\n",
    "        results = process_function(step)\n",
    "\n",
    "        # Assign results to variables based on the provided variable names\n",
    "        for j, result_var_name in enumerate(result_variable_names):\n",
    "            globals()[result_var_name] = results[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c75d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
